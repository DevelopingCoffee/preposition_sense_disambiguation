\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsfonts, amssymb}
\usepackage{scrpage2}
\usepackage{color}
\usepackage{titlesec}
\pagestyle{scrheadings}
\usepackage{ulem, contour}
\usepackage{hyperref}
\usepackage{listings}

\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{12} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal

\renewcommand{\ULdepth}{1.8pt}
\contourlength{0.8pt}

\newcommand{\customuline}[1]{%
  \uline{\phantom{#1}}%
  \llap{\contour{white}{#1}}%
}

\ihead{Flair}
\chead{Preposition sense disambiguation}
\ohead{Text2Scene}
\cfoot{\pagemark}
\setheadsepline{.5pt}

\definecolor{gray}{rgb}{0.33, 0.33, 0.33}
\definecolor{greengreen}{rgb}{0.0, 0.56, 0.0}
\definecolor{fgreen}{rgb}{0.13, 0.55, 0.13}
\definecolor{grellow}{rgb}{0.68, 1.0, 0.18}
\definecolor{orange}{rgb}{1.0, 0.49, 0.0}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}

%List setup
\lstdefinestyle{python}{
	language     = Python,
	basicstyle   = \small\ttm,
	keywordstyle = \small\color{deepblue}\ttb,
	commentstyle = \color{gray},
	emph={__init__,__contains__,self,encoding},
	emphstyle=\small\ttb\color{deepred},   
	stringstyle=\color{deepgreen}, 
	xleftmargin = \parindent
}
\lstset{
	frame = single,
	language=Python,
	breaklines=true,
	tabsize=4,
	escapeinside={(*@}{@*)}
}

\newcommand\pythonstyle{\lstset{
    frame=single,
	language=Python,
	breaklines=true,
	tabsize=4,
	escapeinside={(*@}{@*)},
    style=python            
}}

% Python environment
\lstnewenvironment{python}[1][]
{
\pythonstyle
\lstset{#1}
}
{}

% Python for external files
\newcommand\pythonexternal[2][]{{
\pythonstyle
\lstinputlisting[#1]{#2}}}

% Python for inline
\newcommand\pythoninline[1]{{\pythonstyle\lstinline!#1!}}


\begin{document}
\tableofcontents
\newpage

\section{Einleitung}
\begin{flushleft}
Im Praktikum \textit{Text2Sene} geht es darum, aus Textbeschreibungen Szenen zu erstellen. Dabei wurde die Arbeit unterteilt; wir beschäftigen uns mit der Thematik der \textit{preposition sense disambiguation} (Sinneszuordnung und -erkennung von Präpositionen). Für diese Aufgabe haben wir verschiedene \textit{State-of-the-Art-Verfahren} begutachtet. Schlussendlich haben wir uns dafür entschieden, einerseits einen \textit{Semi-supervised} Ansatz umzusetzen und daneben KIs mit Hilfe der frameworks FlairNLP, AllenNLP und Huggingface zu programmieren.
\end{flushleft}

\section{FlairNLP}
\begin{flushleft}
FlairNLP ist ein Framework, das speziell für NLP-Aufgaben konzipiert ist. Für unsere Aufgabe nutzen wir einen \textit{text classifier}, der wie der Name sagt, eine Eingabe klassifiziert und dadurch die Präposition dem zugehörigen Sinn zuordnet.
\end{flushleft}

\subsection{Vorgehen}
\begin{flushleft}
Damit einer Präposition ein Sinn zugeordnet werden kann, passieren einige Dinge. Der Hauptteil des Projekts besteht aus einer NLP-KI mit dem Flair-framework.

Damit eine Eingabe verarbeitet werden kann, muss diese zunächst vorbereitet werden. Dazu werden die Daten aus den xml-Dateien gelsen und in einer csv-Datei mit dem entsprechenden Format für Flair abgespeichert. Hierbei wird das standard-Format (\_\_label\_\_$<$label$>$) verwendet. Die Trainingsdaten werden in drei Dateien aufgeteilt, dabei werden 80\% Training, 10\% Dev und weitere 10\% Test zugeschrieben.

Nachdem die Daten im csv-Format vorliegen, wird daraus ein Korpus erstellt. Dazu wird ein \textit{Dictionary} und die entsprechenden Embeddings erstellt. Anschließend wird der \textit{Classifier} erstellt und das Training beginnt.
\end{flushleft}

\subsection{Programm}
\begin{flushleft}
Das Flair-Projekt umfasst vier Dateien:
\end{flushleft}
\begin{itemize}
\item[•] Flair\_prepare.py - Diese Datei dient der Vorbereitung der Daten.
\item[•] Dataset\_class.py - Diese Datei enthält wichtige Klassen auf Basis des FlairNLP-Frameworks zum ertsellen des Korpus.
\item[•] flair\_test\_classification\_model.py - Diese Datei dient zum erstellen des \textit{text classifiers} und des Trainings dessen
\item[•] predict.py - Diese Datei enthält den \textit{predictor}, bzw. dient zum testen des Endproduktes.
\end{itemize}

\subsubsection{Flair\_prepare.py}
\begin{flushleft}
Der erste Schritt befasst sich mit der Vorbereitung der Daten. Wir benutzen für unser Training die Daten des \textbf{SemEval 2007}, die Rund 16.000 Sätze beinhalten zu 34 verschiedenen Prepositionen. Diese Daten sind in xml-Format gegeben, weshalb sie in csv-Format geändert werden müssen.

Als erstes werden die xml-Dateien mit Hilfe der Python-library \textit{xml.etree.ElementTree} ausgelesen. Dabei speichern wir die \textit{senseid} und den Satz in einer Liste. Die \textit{senseid} wird zeitgleich mit dem von Flair benötigten Zusatz \textit{\_\_label\_\_} versehen. Fehlerhafte Einträge - solche, bei denen entweder der Sinn oder der Satz fehlt bzw. fehlerhaft ist - werden zudem aussortiert und deren \textit{instanceid} zur Überprüfung ausgegeben.

Nachdem alle Einträge in die Liste eingefügt wurden, wird diese gemischt und anschließend in \textit{Training}, \textit{Dev} sowie \textit{Test} Dateien \textbf{disjunkt} aufgeteilt (80\% - 10\% - 10\%).
\end{flushleft}

\subsubsection{Dataset\_class.py}
\begin{flushleft}
Die Datei \textit{Dataset\_class.py} enthält die Klassen zum Erstellen des Korpus. Diese sind unverändert aus dem \textbf{GitHub Repository} von Flair übernommen. Für eine genaue Dokumentation dieser beiden Klassen verweisen wir auf die Dokumentation von FlairNLP. Sie sind auf csv-Dateien angepasst.
\end{flushleft}

\subsubsection{Flair\_text\_classification\_model.py}
\begin{flushleft}
Diese datei beinhaltet alle Einstellungen und Bauteile für den \textit{Text classifier}.

\pythonexternal[language=Python, linerange={16-23}]{../flair_sense_disambiguation/Flair_text_classification_model.py}

Als erstes wird der Korpus auf Basis der zuvor erstellten csv-Dateien und der angegebnen \textit{col\_name\_map} erzeugt, sowie ein Dictionary des Korpus errichtet. Flair bietet neben einem csv-Korpus auch andere Formate, wir haben uns aber für csv entschieden, da es ein gängiges Format ist und auch für andere Teilprojekte verwendet werden kann.

\pythonexternal[language=Python, linerange={34-41}]{../flair_sense_disambiguation/Flair_text_classification_model.py}

Nachdem der Korpus erzeugt wurde, werden die Embeddings erzeugt. Hierbei können mehrere Embeddings durch sog. \textit{pool-embeddings} zusammen genutzt werden. Wir haben uns für OneHotEmbeddings und WordEmbeddings (Typ \textit{glove)} entschieden.

\pythonexternal[language=Python, linerange={43-45}]{../flair_sense_disambiguation/Flair_text_classification_model.py}

Nachdem die Embeddings erstellt wurden, kann der Classifier erzeugt werden. Alternativ kann natürlicha uch ein bestehender Classifier geladen werden. Wichtig ist nur, dass dieser Classifier auch die Daten lesen kann, also dem selben Grundaufbau folgt.

\pythonexternal[language=Python, linerange={55-66}]{../flair_sense_disambiguation/Flair_text_classification_model.py}

Bevor das Training startet, wählen wir noch, dass auf einer GPU trainiert werden soll. Da Flair über Torch läuft, kann dies einfach über troch gewählt werden.

Anschließend erstellen wir den Trainer auf Basis des zuvor erzeugten oder geladenen Classifier und Korpus und beginnen das Training. Hierbei können verschiedene Einstellungen vorgenommen werden. Zunächst wird das \textit{output-directory} angegeben., welches wir mit \textit{resources} angegeben haben. In diesem Verzeichnis werden dann logs und die Models abgespeichert. Die \textit{learning rate} haben wir auf dem Sdandartwert belassen, ebenso die beiden weiteren Parameter. Die \textit{patience} kann variabel angepasst werden. Sie verursacht, dass das Training bei zu vielen Epochen ohne Verbesserung \textbf{hintereinander} das Training abgebrochen wird. Je höher die patience, desto mehr Epochen ohne Verbesserung können vorkommen, ohne dass das Training abgebrochen wird. Der letzte Parameter ist die maximale Anzahl an Epochen. Wir haben einige Tausend Epochen trainiert. Dieses Trainign kann aber auch dauern - je Epoche bis zu 40 Sekunden oder mehr, je nach Hardware.
\end{flushleft}

\subsubsection{predict.py}
\begin{flushleft}
Der Predictor dient dazu, die Fertige KI anzuwenden. Das Programm kann dafür auch aus der Konsole mit Übergabeparameter verwendet werden. Der Übergabeparameter muss dabei ein String sein.

Alternativ kann das Programm auch ohne Übergabeparameter ausgeführt werden. In diesem Fall wird eine Reihe an Testdaten predictet.
\end{flushleft}

\section{AllenNLP}

\subsection{Vorgehen}

\subsection{Programm}

\section{Blabla}

\end{document}